{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "### Interactive medword testing tool ###\n",
    "\n",
    "import medword_pipeline\n",
    "import embedding_fasttext\n",
    "import embedding_word2vec\n",
    "import embedding_word2vec_composite\n",
    "import shared.load_config \n",
    "import json\n",
    "import pylab\n",
    "import os\n",
    "import model_validation as mv\n",
    "import preprocess as pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### define a new configuration\n",
    "config_dict = {\n",
    "    \n",
    "    # changes base data directory for developing on smaller data.\n",
    "    # possible values: \"develop\" and \"normal\"\n",
    "    \"running_mode\": \"develop\",\n",
    "    \n",
    "    # if you want to compute new training data from raw-data\n",
    "    \"compute_new_data\": False,\n",
    "    # note: if you don't compute new training data, you must provide them\n",
    "    #       at train_data_src for training a new model\n",
    "    \n",
    "    # if you want to train a new model on training data\n",
    "    \"train_new_model\": False,\n",
    "    # note: if you don't train a new model, you must provide it\n",
    "    #       at emb_model_src for the validation step\n",
    "    \n",
    "    # if you want to run the validation\n",
    "    \"run_validation\": False,\n",
    "    \n",
    "    # chose embedding method. possible values: \"word2vec\" and \"fasttext\"\n",
    "    \"embedding_method\": \"word2vec-composite\",\n",
    "    \n",
    "    # chose embedding algorithm. possible values: \"skipgram\" and \"cbow\"\n",
    "    \"embedding_algorithm\": \"skipgram\",\n",
    "    \n",
    "    # chose filenames for train data and embedding model\n",
    "    \"train_data_filename\": \"train.txt\",\n",
    "    \"embedding_model_filename\": \"emb_model_ft.bin\",\n",
    "    \n",
    "    # base data dir when running in \"normal\" mode\n",
    "    \"base_data_dir\": \"data/\",\n",
    "    \n",
    "    # base data dir when running in \"develop\" mode\n",
    "    \"develop_base_data_dir\": \"dev_data/\",\n",
    "    \n",
    "    # embedding model settings\n",
    "    \"embedding_vector_dim\": 200,\n",
    "    \"min_token_appearance\": 5,\n",
    "    \n",
    "    # chose tokenizer. possible values: \"nst\" and \"sgt\"\n",
    "    ## NonStemmingTokenizer: 'nst'\n",
    "    # - no stemming, only remove punctuation marks\n",
    "    # - lowercase letters\n",
    "\n",
    "    ## SimpleGermanTokenizer: 'sgt'\n",
    "    # - remove punctuation marks\n",
    "    # - stemming\n",
    "    # - lowercase letters\n",
    "    \"tokenizer\": \"nst\",\n",
    "    \n",
    "    # validation data settings\n",
    "    \"val_data_dir\": \"data/validation_data/\",\n",
    "    \"synonyms_numb_closest_vec\": 40,\n",
    "    \"doesntfit_file\": \"german_doesntfit1.txt\",\n",
    "    \"synonyms_file\": \"german_synonyms3.txt\",\n",
    "    \n",
    "    # where to store the configuration file\n",
    "    \"config_path\": \"configuration-ipython.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### save config file (don't change)\n",
    "config_src = config_dict[\"config_path\"]\n",
    "\n",
    "with open(config_src, 'w+') as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "    \n",
    "# load config object based on config file (don't change)\n",
    "config = shared.load_config.Configuration(config_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### initialize embedding object (don't change)\n",
    "if config.config['embedding_method'] == 'fasttext':\n",
    "    embedding = embedding_fasttext.EmbeddingFasttext(config)\n",
    "\n",
    "elif config.config['embedding_method'] == 'word2vec':\n",
    "    embedding = embedding_word2vec.EmbeddingWord2vec(config)\n",
    "\n",
    "elif config.config['embedding_method'] == 'word2vec-composite':\n",
    "    embedding = embedding_word2vec_composite.EmbeddingWord2vecComposite(config)\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('embedding_algorithm (in config) must be \"fasttext\" or \"word2vec\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Running in NORMAL mode.\n"
     ]
    }
   ],
   "source": [
    "### run pipeline for that embedding (create train data, train model, validate model)\n",
    "medword_pipeline.run_pipeline(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train or load a model.\n"
     ]
    }
   ],
   "source": [
    "### Play with embedding model ###\n",
    "#\n",
    "# The following parts are intended to get familiar with the \n",
    "# model and the data\n",
    "\n",
    "# check if model is instantiatet\n",
    "if embedding._model is None: \n",
    "    print(\"Train or load a model.\")\n",
    "else: \n",
    "    print(\"Model is present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# maybe load model (if not created in training or loaded in validation of pipeline)\n",
    "# Note: The model must fit the embedding method (fasttext or word2vec)!\n",
    "emb_model_dir = 'dev_data/embeddings/'\n",
    "emb_model_filename = 'emb_model_ft.bin'\n",
    "\n",
    "embedding.load_model(emb_model_dir, emb_model_filename)\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 189\n",
      "vector dim: 200\n"
     ]
    }
   ],
   "source": [
    "### Get model information ###\n",
    "print(\"vocab size:\", len(embedding.get_vocab()))\n",
    "print(\"vector dim:\", embedding.vec_dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word is in model vocab:  False\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'blinddarm entzuendung'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8573e9f42d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# create list of neighbours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nearest_neighbours\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/medword/embedding_word2vec_composite.py\u001b[0m in \u001b[0;36mmost_similar_n\u001b[0;34m(self, word, topn)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mReferenceError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/word2vec/wordvectors.py\u001b[0m in \u001b[0;36mcosine\u001b[0;34m(self, word, n)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;36m2.\u001b[0m \u001b[0mcosine\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mbest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/word2vec/wordvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/word2vec/wordvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/word2vec/wordvectors.py\u001b[0m in \u001b[0;36mix\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mon\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_hash\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'blinddarm entzuendung'"
     ]
    }
   ],
   "source": [
    "### Play with model ###\n",
    "\n",
    "# define a word from which you like to see it's n-nearest neighbours \n",
    "# in the embedding space\n",
    "word = 'blinddarm entzuendung'\n",
    "n_nearest_neighbours = 10\n",
    "\n",
    "# check if word is in model vocab\n",
    "print(\"word is in model vocab: \", word in embedding.get_vocab())\n",
    "\n",
    "# create list of neighbours\n",
    "embedding.most_similar_n(word, n_nearest_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gebaermutterhalsentzuendung', 0.8089451789855957),\n",
       " ('gebaermutterentzuendung', 0.8057259321212769),\n",
       " ('vulvaentzuendung', 0.7802436351776123),\n",
       " ('ovar-entzuendung', 0.7689380645751953),\n",
       " ('ovarentzuendung', 0.7654260993003845),\n",
       " ('hodenentzuendung', 0.759384036064148),\n",
       " ('mammaentzuendung', 0.753741443157196),\n",
       " ('tuboovarialentzuendung', 0.7529478073120117),\n",
       " ('uterusentzuendung', 0.7498918771743774),\n",
       " ('penisentzuendung', 0.7436195611953735)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Analogy\n",
    "\n",
    "# embedding.analogy([positives], [negatives], topn)\n",
    "embedding.analogy(['entzuendung', 'frau'], [], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "der\n",
      "der\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/medword/embedding_word2vec_composite.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if vector == None:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.22354685, -0.16270085,  0.17073798,  0.0319031 , -0.11392989,\n",
       "       -0.08383776,  0.11280128, -0.15535317, -0.10614239,  0.24326162])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a vector of a word\n",
    "import numpy as np\n",
    "vec = embedding.word_vec('derderfasdf')\n",
    "vec[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No word found to show.\n"
     ]
    }
   ],
   "source": [
    "### Visualization ###\n",
    "# From a list of words, plot all words and it's neighbours \n",
    "\n",
    "# define the words you would like to visualize\n",
    "word_list = ['r651', 'sirs mit organkomplikation', 'r650']\n",
    "\n",
    "# define the number of closest neighbors to display per word\n",
    "n_nearest_neighbours = 4\n",
    "\n",
    "# change figure size if desired\n",
    "width = 10.0\n",
    "height = 8.0\n",
    "pylab.rcParams['figure.figsize'] = (width, height)\n",
    "\n",
    "# plotting\n",
    "mv.visualize_words(embedding, word_list, n_nearest_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sternotomie', '35h111']\n"
     ]
    }
   ],
   "source": [
    "### test a tokenizer for the prepocessing and validation ###\n",
    "\n",
    "## NonStemmingTokenizer: 'nst'\n",
    "# - no stemming, only remove punctuation marks\n",
    "# - lowercase letters\n",
    "config.config['tokenizer'] = 'nst'\n",
    "\n",
    "## SimpleGermanTokenizer: 'sgt'\n",
    "# - remove punctuation marks\n",
    "# - stemming\n",
    "# - lowercase letters\n",
    "# config.config['tokenizer'] = 'sgt'\n",
    "\n",
    "# test the chosen tokenizer\n",
    "tokenizer = pp.get_tokenizer(config)\n",
    "example = \"sternotomie 35h111\"\n",
    "             \n",
    "tk_example = tokenizer.tokenize(str(example))\n",
    "\n",
    "print(tk_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
